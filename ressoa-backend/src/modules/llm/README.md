# LLM Module - Abstra√ß√£o Multi-Provider com Versionamento de Prompts

**Story:** 5.1 - Backend LLM Service Abstraction & Prompt Versioning
**Epic:** 5 - AI Analysis Pipeline (Foundation)

## üìã Vis√£o Geral

Este m√≥dulo fornece uma camada de abstra√ß√£o para Large Language Models (LLMs) com suporte a m√∫ltiplos providers e sistema de versionamento de prompts com A/B testing.

**Prop√≥sito:**
- ‚úÖ Preven√ß√£o de vendor lock-in (multi-provider)
- ‚úÖ Otimiza√ß√£o de custos (escolha do melhor provider por caso de uso)
- ‚úÖ Melhoria cont√≠nua de qualidade (A/B testing de prompts)
- ‚úÖ Rastreabilidade de custos por escola

## üèóÔ∏è Arquitetura

```
src/modules/llm/
‚îú‚îÄ‚îÄ interfaces/
‚îÇ   ‚îî‚îÄ‚îÄ llm-provider.interface.ts    # Interface comum para LLM providers
‚îú‚îÄ‚îÄ providers/
‚îÇ   ‚îú‚îÄ‚îÄ claude.provider.ts           # Claude 4.6 Sonnet (an√°lise pedag√≥gica)
‚îÇ   ‚îî‚îÄ‚îÄ gpt.provider.ts              # GPT-4.6 mini (exerc√≠cios contextuais)
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îî‚îÄ‚îÄ prompt.service.ts            # Versionamento & A/B testing
‚îú‚îÄ‚îÄ llm.module.ts                    # Configura√ß√£o DI
‚îî‚îÄ‚îÄ README.md                        # Este arquivo
```

## üîå Providers Dispon√≠veis

### Claude Sonnet 4.6 (An√°lise Pedag√≥gica)
- **Modelo:** `claude-sonnet-4-20250514`
- **Uso:** An√°lise pedag√≥gica profunda (cobertura BNCC, qualidade, alertas)
- **Custo:** $3/1M input tokens, $15/1M output tokens
- **Token Name:** `CLAUDE_PROVIDER`

### GPT-4.6 mini (Exerc√≠cios Contextuais)
- **Modelo:** `gpt-4o-mini`
- **Uso:** Gera√ß√£o de exerc√≠cios contextuais
- **Custo:** $0.15/1M input tokens, $0.60/1M output tokens
- **Token Name:** `GPT_PROVIDER`

## ‚ûï Como Adicionar Novos Providers

Para adicionar um novo LLM provider (ex: GeminiProvider para fallback), siga estes passos:

### 1. Instalar SDK do Provider

```bash
npm install @google/generative-ai
```

### 2. Criar Provider Class

Crie o arquivo `src/modules/llm/providers/gemini.provider.ts` seguindo o padr√£o de ClaudeProvider/GPTProvider. Implemente a interface `LLMProvider` com os m√©todos:
- `getName()`: Retorna `ProviderLLM.GEMINI_PRO`
- `generate(prompt, options)`: Chama API, calcula custos, retorna `LLMResult`
- `isAvailable()`: Health check

**IMPORTANTE:**
- Adicionar structured logging em todos m√©todos
- Calcular custos usando f√≥rmula: `(tokens / 1_000_000) * pre√ßo_por_milh√£o`
- Adicionar coment√°rios inline nas f√≥rmulas de custo (ex: "// Input: $0.50/1M tokens")
- Error handling com contexto: `throw new Error(\`GeminiProvider: Falha - \${error.message}\`)`

### 3. Registrar Provider no LLMModule

```typescript
// src/modules/llm/llm.module.ts
import { GeminiProvider } from './providers/gemini.provider';

@Module({
  providers: [
    // Existing providers
    { provide: 'CLAUDE_PROVIDER', useClass: ClaudeProvider },
    { provide: 'GPT_PROVIDER', useClass: GPTProvider },

    // ‚úÖ New provider
    { provide: 'GEMINI_PROVIDER', useClass: GeminiProvider },

    PromptService,
  ],
  exports: ['CLAUDE_PROVIDER', 'GPT_PROVIDER', 'GEMINI_PROVIDER', PromptService],
})
export class LLMModule {}
```

### 4. Adicionar Vari√°vel de Ambiente

```env
# .env
GEMINI_API_KEY=AIzaSy...
```

### 5. Criar Unit Tests

Crie `gemini.provider.spec.ts` seguindo o padr√£o dos outros providers:
- Mock do SDK usando `jest.mock('@google/generative-ai')`
- Testar `getName()`, `generate()`, `isAvailable()`
- Testar c√°lculo de custo com diferentes token counts
- Testar error handling
- Coverage >80%

### 6. Atualizar Prisma Schema (se necess√°rio)

Se o `ProviderLLM` enum n√£o tem GEMINI_PRO:

```prisma
enum ProviderLLM {
  CLAUDE_SONNET
  CLAUDE_HAIKU
  GPT4_TURBO
  GPT4_MINI
  GEMINI_PRO      // ‚úÖ Adicionar
  GEMINI_FLASH
}
```

Rodar: `npx prisma migrate dev --name add-gemini-enum`

### 7. Atualizar Documenta√ß√£o

- Adicionar GeminiProvider √† se√ß√£o "Providers Dispon√≠veis" deste README
- Incluir modelo, uso recomendado, custo, token name

---

## üíâ Uso (Dependency Injection)

### Injetando um Provider Espec√≠fico

```typescript
import { Injectable, Inject } from '@nestjs/common';
import { LLMProvider } from '../llm/interfaces';

@Injectable()
export class AnaliseService {
  constructor(
    @Inject('CLAUDE_PROVIDER') private claude: LLMProvider,
    @Inject('GPT_PROVIDER') private gpt: LLMProvider,
  ) {}

  async gerarAnalise(transcricao: string) {
    // Usar Claude para an√°lise pedag√≥gica
    const result = await this.claude.generate(transcricao, {
      temperature: 0.7,
      maxTokens: 4000,
      systemPrompt: 'Voc√™ √© um assistente pedag√≥gico...',
    });

    console.log(`Custo: $${result.custo_usd.toFixed(6)}`);
    console.log(`Tempo: ${result.tempo_processamento_ms}ms`);

    return result.texto;
  }
}
```

### Injetando o PromptService

```typescript
import { Injectable } from '@nestjs/common';
import { PromptService } from '../llm/services/prompt.service';

@Injectable()
export class PipelineService {
  constructor(private promptService: PromptService) {}

  async executarPrompt(nomePrompt: string, variaveis: Record<string, any>) {
    // Recupera prompt ativo (com A/B testing se configurado)
    const prompt = await this.promptService.getActivePrompt(nomePrompt);

    // Renderiza template com vari√°veis
    const promptRendered = await this.promptService.renderPrompt(prompt, variaveis);

    return promptRendered;
  }
}
```

## üìù Sistema de Versionamento de Prompts

### Estrutura de um Prompt

```typescript
{
  id: "uuid",
  nome: "prompt-cobertura",       // Nome √∫nico do prompt
  versao: "v1.1.0",               // Semantic versioning
  conteudo: "Analise: {{transcricao}} para {{habilidade}}",
  variaveis: {                    // Metadata das vari√°veis esperadas
    transcricao: "string",
    habilidade: "string"
  },
  modelo_sugerido: "CLAUDE_SONNET", // Provider recomendado
  ativo: true,                    // Se est√° dispon√≠vel para uso
  ab_testing: true,               // Se faz parte de A/B test
  created_at: "2026-02-11T...",
  updated_at: "2026-02-11T..."
}
```

### Workflow de Versionamento

1. **Criar Vers√£o Inicial (v1.0.0)**
   ```typescript
   await promptService.createPrompt({
     nome: 'prompt-cobertura',
     versao: 'v1.0.0',
     conteudo: 'Analise a cobertura BNCC: {{transcricao}}',
     variaveis: { transcricao: 'string' },
     modelo_sugerido: ProviderLLM.CLAUDE_SONNET,
     ativo: true,
     ab_testing: false,
   });
   ```

2. **Criar Nova Vers√£o com A/B Testing (v1.1.0)**
   ```typescript
   await promptService.createPrompt({
     nome: 'prompt-cobertura',
     versao: 'v1.1.0',
     conteudo: 'Analise MELHORADA: {{transcricao}} e {{planejamento}}',
     variaveis: { transcricao: 'string', planejamento: 'string' },
     ativo: true,
     ab_testing: true, // ‚úÖ Ativa A/B test 50/50 com v1.0.0
   });
   ```

3. **Uso com A/B Testing Autom√°tico**
   ```typescript
   // Retorna v1.0.0 ou v1.1.0 aleatoriamente (50/50)
   const prompt = await promptService.getActivePrompt('prompt-cobertura');
   ```

4. **Ap√≥s Valida√ß√£o - Desativar Vers√£o Antiga**
   ```typescript
   await promptService.updatePromptStatus('prompt-cobertura', 'v1.0.0', {
     ativo: false,
   });

   // Agora sempre retorna v1.1.0
   const prompt = await promptService.getActivePrompt('prompt-cobertura');
   ```

## üß™ A/B Testing

### Como Funciona

- **2 vers√µes ativas + `ab_testing=true` na mais recente** ‚Üí Split 50/50 aleat√≥rio
- **1 vers√£o ativa** ‚Üí Sempre retorna essa vers√£o
- **2 vers√µes ativas + `ab_testing=false`** ‚Üí Sempre retorna a mais recente

### M√©tricas para Avaliar Prompts

```typescript
// Ap√≥s an√°lise ser aprovada pelo professor
await prisma.analise.update({
  where: { id: analiseId },
  data: {
    aprovada: true,
    prompt_versao_usada: prompt.versao, // Rastrear qual vers√£o foi usada
    tempo_revisao_segundos: 120,
  },
});

// Query para medir qualidade de prompts
const metricas = await prisma.analise.groupBy({
  by: ['prompt_versao_usada'],
  where: { created_at: { gte: ultimos30Dias } },
  _count: true,
  _avg: { tempo_revisao_segundos: true },
  _sum: { aprovada: true },
});

// Calcular taxa de aprova√ß√£o por vers√£o
metricas.forEach(m => {
  const taxaAprovacao = (m._sum.aprovada / m._count) * 100;
  console.log(`Vers√£o ${m.prompt_versao_usada}: ${taxaAprovacao}% aprova√ß√£o`);
});
```

## üí∞ Rastreamento de Custos

Todos os providers retornam `LLMResult` com metadados de custo:

```typescript
interface LLMResult {
  texto: string;
  provider: ProviderLLM;      // CLAUDE_SONNET | GPT4_MINI
  modelo: string;             // claude-sonnet-4 | gpt-4o-mini
  tokens_input: number;
  tokens_output: number;
  custo_usd: number;          // ‚ö†Ô∏è CR√çTICO - rastreamento por escola
  tempo_processamento_ms: number;
  metadata?: Record<string, any>;
}
```

**Exemplo de logging de custos:**

```typescript
const result = await this.claude.generate(prompt);

this.logger.log({
  message: 'LLM call completed',
  escola_id: escolaId,
  provider: result.provider,
  custo_usd: result.custo_usd,
  tokens_total: result.tokens_input + result.tokens_output,
});

// Agregar custos por escola para billing
await this.custoService.registrarChamadaLLM({
  escola_id: escolaId,
  provider: result.provider,
  custo_usd: result.custo_usd,
  data: new Date(),
});
```

## üîß Configura√ß√£o

### Vari√°veis de Ambiente

```env
# Anthropic API Key (Claude)
ANTHROPIC_API_KEY=sk-ant-api03-...

# OpenAI API Key (GPT)
OPENAI_API_KEY=sk-proj-...
```

### Importa√ß√£o no AppModule

```typescript
import { LLMModule } from './modules/llm/llm.module';

@Module({
  imports: [
    // ...
    LLMModule,
    // ...
  ],
})
export class AppModule {}
```

## üß™ Testes

### Unit Tests
```bash
# Testar ClaudeProvider
npm test -- claude.provider.spec.ts

# Testar GPTProvider
npm test -- gpt.provider.spec.ts

# Testar PromptService
npm test -- prompt.service.spec.ts
```

### E2E Tests
```bash
# Testar fluxo completo de versionamento e A/B testing
npm run test:e2e -- llm-prompt-versioning.e2e-spec.ts
```

**Cobertura:** >80% em todos os providers e services

## üìä Health Checks

Verificar disponibilidade dos providers:

```typescript
const claudeDisponivel = await this.claude.isAvailable();
const gptDisponivel = await this.gpt.isAvailable();

if (!claudeDisponivel) {
  this.logger.error('Claude provider indispon√≠vel - usar fallback');
}
```

## üöÄ Pr√≥ximos Passos (Epics Futuros)

- **Story 5.2:** Pipeline serial de 5 prompts orquestrados
- **Story 5.3:** Prompts 1-2 (Cobertura BNCC + An√°lise Qualitativa)
- **Story 5.4:** Prompts 3-4 (Relat√≥rio + Exerc√≠cios)
- **Story 5.5:** Prompt 5 + Analysis Worker + Alertas

## üìö Refer√™ncias

- [Estrat√©gia de Prompts IA](../../../../_bmad-output/planning-artifacts/estrategia-prompts-ia-2026-02-08.md)
- [Architecture Decision Document](../../../../_bmad-output/planning-artifacts/architecture.md) - Decis√µes #4, #5, #7
- [Story 4.1 - STT Service Abstraction](../../../../_bmad-output/implementation-artifacts/4-1-backend-stt-service-abstraction-layer.md) - Pattern reference

## ‚ö†Ô∏è Notas Importantes

1. **NUNCA** chamar LLMs sem rastrear custo (`custo_usd`) - impacta billing por escola
2. **SEMPRE** usar `PromptService.getActivePrompt()` - nunca query direta no Prisma
3. **Vari√°veis faltando** em templates s√£o deixadas como `{{key}}` para debugging
4. **Logging estruturado** √© obrigat√≥rio (provider, custo, tokens, tempo)
5. **API timeouts:** 2 minutos para LLM calls (vs 5min para STT)

---

**√öltima Atualiza√ß√£o:** 2026-02-12
**Autor:** Dev Agent (Story 5.1)
**Modelo Usado:** Claude Sonnet 4.5
